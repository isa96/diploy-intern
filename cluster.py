# -*- coding: utf-8 -*-
"""cluster.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yNub03d68WBIVn8m0laNB10UEL0oGgWD

# KMeans
"""

import warnings

import datetime as dt

from operator import attrgetter

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import seaborn as sns

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import davies_bouldin_score, silhouette_score

import squarify

from mlxtend.frequent_patterns import apriori

from yellowbrick.cluster import KElbowVisualizer
from yellowbrick.cluster import SilhouetteVisualizer


warnings.filterwarnings("ignore")

data = pd.read_csv("clean-data.csv", sep=",")
data.head()

data["date"] = pd.to_datetime(data["date"])
data["qty"] = data["qty"].map(float)
data.info()

data.isnull().sum()

data.isna().values.any()

data[data.duplicated()]

plt.figure(figsize=(15, 6))
sns.heatmap(data.corr(), annot=True)
plt.plot()

df_clust = data.groupby("customerid").agg(
    {"transactionid": "count", "qty": "sum", "totalamount": "sum"}
)

df_clust.head()

print(df_clust.shape)

df_clust.describe()

features = df_clust.columns
fig, ax = plt.subplots(1, len(df_clust.columns), figsize=(12, 5))
for i in range(0, len(df_clust.columns)):
    sns.boxplot(data=df_clust, y=features[i], ax=ax[i])
plt.tight_layout()
plt.show()

X = df_clust.values
X_std = StandardScaler().fit_transform(X)
df_std = pd.DataFrame(data=X_std, columns=df_clust.columns)
df_std.isna().sum()

inertia = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init="k-means++", random_state=42)
    kmeans.fit(df_std.values)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(10, 6))
plt.xticks(list(range(1, 11)))
plt.plot(range(1, 11), inertia, marker="o")
plt.title("Elbow Method")
plt.show()

k_range = range(2, 6)
fig, axes = plt.subplots(4, 1, figsize=(10, 18))

for i in k_range:
    model = KMeans(i, init="k-means++", n_init=100, random_state=42)
    visualizer = SilhouetteVisualizer(model, ax=axes[i - 2])
    visualizer.fit(df_std.values)
    visualizer.finalize()
    axes[i - 2].set_xlim(-0.1, 1)

plt.tight_layout()

Elbow_M = KElbowVisualizer(
    KMeans(init="k-means++", n_init=100, random_state=42), k=((1, 11))
)
Elbow_M.fit(df_std)
Elbow_M.show()

kmeans_3 = KMeans(n_clusters=3, init="k-means++", n_init=100, random_state=42)
kmeans_3.fit(X_std)

df_cl3 = pd.DataFrame(data=X_std, columns=df_clust.columns)
df_cl3["cluster"] = kmeans_3.labels_
df_cl3.sample(3)

plt.figure(figsize=(15, 6))
sns.scatterplot(x="qty", y="totalamount", data=df_cl3, hue="cluster")
plt.show()

print(silhouette_score(X_std, kmeans_3.labels_))
print(davies_bouldin_score(X_std, kmeans_3.labels_))

print(silhouette_score(df_clust, kmeans_3.labels_))
print(davies_bouldin_score(df_clust, kmeans_3.labels_))

"""# PCA"""

pca = PCA(n_components=2)
pca_data = pca.fit_transform(X_std)
pca_df = pd.DataFrame.from_records(data=pca_data, columns=["x1", "x2"])
kmeans_pca = KMeans(n_clusters=3, init="k-means++", n_init=100, random_state=42)
kmeans_pca.fit(pca_data)
pca_df["cluster"] = kmeans_pca.labels_

pca_df.head()

plt.figure(figsize=(15, 6))
sns.scatterplot(x="x1", y="x2", data=pca_df, hue="cluster")
plt.show()

x1 = pca_df.drop(columns="cluster")
y1 = pca_df["cluster"]

print(silhouette_score(x1, y1))
print(davies_bouldin_score(x1, y1))

print(silhouette_score(df_clust, y1))
print(davies_bouldin_score(df_clust, y1))

"""# Note:
1. Tujuan Silhouette Score adalah untuk mendapatkan nilai yang tinggi, mendekati 1, yang menandakan bahwa klaster adalah "padat" dan berbeda secara signifikan dari klaster lain. Sebaliknya, nilai negatif mendekati -1 menandakan bahwa objek lebih cenderung ditempatkan dalam klaster yang salah, dan nilai dekat 0 menandakan bahwa objek berada di dekat batas klaster.
2. Tujuan Davies-Bouldin Score adalah untuk mendapatkan nilai yang rendah, mendekati nol, yang menandakan bahwa klaster memiliki batas yang jelas dan terpisah satu sama lain, dan centroid berada cukup dekat dengan anggota klaster. Semakin rendah nilai skornya, semakin baik kualitas klasteringnya.

# RFM Analysis
"""

today_date = dt.datetime(2023, 1, 1)
rfm = data.groupby("customerid").agg(
    {
        "date": lambda x: (today_date - x.max()).days,
        "transactionid": "count",
        "totalamount": "sum",
    }
)

rfm.rename(
    columns={
        "date": "Recency",
        "transactionid": "Frequency",
        "totalamount": "Monetary",
    },
    inplace=True,
)
rfm.head()

r_labels = range(4, 0, -1)
f_labels = range(1, 5)
m_labels = range(1, 5)

r_quartiles = pd.qcut(rfm["Recency"], q=4, labels=r_labels)
f_quartiles = pd.qcut(rfm["Frequency"], q=4, labels=f_labels)
m_quartiles = pd.qcut(rfm["Monetary"], q=4, labels=m_labels)

rfm = rfm.assign(R=r_quartiles, F=f_quartiles, M=m_quartiles)
rfm.head()


def seg(dataf):
    """seg

    Args:
        data (dataframe): dataframe have value retency, frequency, and monetary

    Returns:
        sum of string values from RFM: combine values from value r, f, and m
    """
    return str(dataf["R"]) + str(dataf["F"]) + str(dataf["M"])


rfm["RFM"] = rfm.apply(seg, axis=1)
rfm.head()

rfm["RFM_Score"] = rfm[["R", "F", "M"]].sum(axis=1)
rfm.head()

x = ["Recency", "Frequency", "Monetary"]
f, ax = plt.subplots(figsize=(8, 6))

sns.boxplot(data=rfm[x], palette="Set2")
plt.yscale("log")
plt.title("Outliers Variable Distribution", fontsize=12)
plt.xlabel("RFM")
plt.ylabel("Range")
plt.show()

rfm.reset_index(inplace=True)
rfm.tail()


def segments(dataf):
    """segment

    Args:
        data (dataframe): contain value of RFM score from dataframe

    Returns:
        string: value from segmentation based on rfm score
    """
    if dataf["RFM_Score"] > 10:
        return "champions"

    if (dataf["RFM_Score"] > 8) and (dataf["RFM_Score"] <= 10):
        return "potential_loyalists"

    if dataf["RFM_Score"] == 8:
        return "need_attention"

    if dataf["RFM_Score"] == 7:
        return "about_to_sleep"

    if (dataf["RFM_Score"] > 4) and (dataf["RFM_Score"] < 7):
        return "at_risk"

    return "hibernating"


rfm["Segment"] = rfm.apply(segments, axis=1)
rfm.head()

rfm.groupby(["Segment"]).agg(
    {
        "customerid": "count",
        "Recency": "mean",
        "Frequency": "mean",
        "Monetary": "mean",
        "RFM_Score": "mean",
    }
).sort_values(by="RFM_Score", ascending=False).round(2)

segments = rfm["Segment"].value_counts(normalize=True).sort_values(ascending=False)
fig = plt.gcf()
ax = fig.add_subplot()
fig.set_size_inches(8, 6)

squarify.plot(
    sizes=segments,
    label=[
        "at_risk",
        "potential_loyalists",
        "hibernating",
        "champions",
        "about_to_sleep",
        "need_attention",
    ],
    color=sns.color_palette("Set2"),
    bar_kwargs={"alpha": 0.7},
)
plt.title("Customer Segmentation Map", fontsize=12)
plt.show()

print(" RFM Model Evaluation ".center(70, "="))
X = rfm[["Recency", "Frequency"]]
labels = rfm["Segment"]
print(f"Number of Observations: {X.shape[0]}")
print(f"Number of Segments: {labels.nunique()}")
print(f"Silhouette Score: {round(silhouette_score(X, labels), 3)}")
print(f'Davies Bouldin Score: {round(davies_bouldin_score(X, labels), 3)} \n{70*"="}')

plt.figure(figsize=(18, 8))
ax = sns.countplot(data=rfm, x="Segment")
total = len(rfm.Segment)
for patch in ax.patches:
    FORMULA = 100 * patch.get_height() / total
    PERCENTAGE = f"{FORMULA:.1f}%"
    x = patch.get_x() + patch.get_width() / 2 - 0.17
    y = patch.get_y() + patch.get_height() * 1.005
    ax.annotate(PERCENTAGE, (x, y), size=14)
plt.title("Number of Customers by Segments", size=16)
plt.xlabel("Segment", size=14)
plt.ylabel("Count", size=14)
plt.xticks(size=10)
plt.yticks(size=10)
plt.show()

rfm_RFM = rfm[["Recency", "Frequency", "Monetary"]]

scaler = StandardScaler()
rfm_standard = scaler.fit_transform(rfm_RFM)
rfm_standard = pd.DataFrame(rfm_standard)
rfm_standard.columns = ["Recency", "Frequency", "Monetary"]
rfm_standard.head()

inertia = []

for i in range(1, 11):
    kmeans_6 = KMeans(n_clusters=i, init="k-means++", n_init=100, random_state=42)
    kmeans_6.fit(rfm_standard)
    inertia.append(kmeans_6.inertia_)

plt.figure(figsize=(10, 6))
plt.xticks(list(range(1, 11)))
plt.plot(range(1, 11), inertia, marker="o")
plt.title("Elbow Method")
plt.show()

kmeans_rfm = KMeans(n_clusters=3, init="k-means++", n_init=100, random_state=42)
kmeans_rfm.fit(rfm_standard)
cluster_labels = kmeans_rfm.labels_

rfm_k3 = rfm.assign(Cluster=cluster_labels)
rfm_k3.head()

rfm_k3.groupby("Cluster").agg(
    {"Recency": "mean", "Frequency": "mean", "Monetary": "mean", "RFM_Score": "mean"}
).round(1)

rfm.groupby(["Segment"]).agg(
    {
        "customerid": "count",
        "Recency": "mean",
        "Frequency": "mean",
        "Monetary": "mean",
        "RFM_Score": "mean",
    }
).sort_values(by="RFM_Score", ascending=False)

rfm_k3.groupby(["Cluster", "Segment"]).agg(
    {"customerid": "count", "RFM_Score": "mean"}
).round(2)

rfm_standard["customerid"] = rfm["customerid"]
rfm_standard = rfm_standard.assign(Cluster=cluster_labels)
rfm_standard.head()

fig, axes = plt.subplots(1, 3, figsize=(12, 5))

sns.boxplot(data=rfm_standard, x="Cluster", y="Recency", palette="Set2", ax=axes[0])
sns.boxplot(data=rfm_standard, x="Cluster", y="Frequency", palette="Set2", ax=axes[1])
sns.boxplot(data=rfm_standard, x="Cluster", y="Monetary", palette="Set2", ax=axes[2])

plt.show()

rfm_k3.head()

rfm_standard.head()

rfm_k4 = rfm_k3.drop(columns=["customerid", "RFM_Score"])
plt.figure(figsize=(15, 6))
sns.pairplot(data=rfm_k4, hue="Cluster")
plt.show()

x2 = rfm_standard.drop(columns=["customerid", "Cluster"])
y2 = rfm_standard["Cluster"]

print(silhouette_score(x2, y2))
print(davies_bouldin_score(x2, y2))

x3 = rfm_k3.drop(columns=["customerid", "Cluster", "Segment"])
y3 = rfm_k3["Cluster"]

print(silhouette_score(x3, y3))
print(davies_bouldin_score(x3, y3))

"""# Cohort Analysis"""


def cohort_analysis(dataframe):
    """cohort_analysis

    Args:
        dataframe (pd.Dataframe): contain all information from dataset
    """
    daf = dataframe.copy()
    daf = daf[["customerid", "transactionid", "date"]].drop_duplicates()
    daf["order_month"] = daf["date"].dt.to_period("M")
    daf["cohort"] = daf.groupby("customerid")["date"].transform("min").dt.to_period("M")
    cohort_data = (
        daf.groupby(["cohort", "order_month"])
        .agg(n_customers=("customerid", "nunique"))
        .reset_index(drop=False)
    )
    cohort_data["period_number"] = (cohort_data.order_month - cohort_data.cohort).apply(
        attrgetter("n")
    )
    cohort_pivot = cohort_data.pivot_table(
        index="cohort", columns="period_number", values="n_customers"
    )
    cohort_size = cohort_pivot.iloc[:, 0]
    retention_matrix = cohort_pivot.divide(cohort_size, axis=0)
    with sns.axes_style("white"):
        fig1, ax1 = plt.subplots(
            1, 2, figsize=(12, 8), sharey=True, gridspec_kw={"width_ratios": [1, 11]}
        )
        sns.heatmap(
            retention_matrix,
            mask=retention_matrix.isnull(),
            annot=True,
            cbar=False,
            fmt=".0%",
            cmap="coolwarm",
            ax=ax1[1],
        )
        ax1[1].set_title("Monthly Cohorts: User Retention", fontsize=14)
        ax1[1].set(xlabel="# of periods", ylabel="")
        white_cmap = mcolors.ListedColormap(["white"])
        sns.heatmap(
            pd.DataFrame(cohort_size).rename(columns={0: "cohort_size"}),
            annot=True,
            cbar=False,
            fmt="g",
            cmap=white_cmap,
            ax=ax1[0],
        )
        fig1.tight_layout()


cohort_analysis(data)

"""# Market Basket Analysis"""


def create_transaction_product_df(dataframe):
    """create_invoice_product_df

    Args:
        dataframe (pd.Dataframe): contain all information from dataset

    Returns:
        pd.Dataframe: group value transactionid and product_name based on quantity
    """
    return (
        dataframe.groupby(["transactionid", "product_name"])["qty"]
        .sum()
        .unstack()
        .fillna(0)
        .applymap(lambda x: 1 if x > 0 else 0)
    )


pro_df = create_transaction_product_df(data)
pro_df.head()

frequent_itemsets = apriori(pro_df, min_support=0.01, use_colnames=True)
frequent_itemsets.sort_values("support", ascending=False)
